

# import asyncio
# from playwright.async_api import async_playwright
# from urllib.parse import urljoin
# import logging
# import time
# from datetime import timedelta
# from fake_useragent import UserAgent
# import aiohttp
# import argparse
# import sys
#
# # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
# logger = logging.getLogger(__name__)
#
#
# def setup_logging(log_file=None):
#     """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∏—Å—Ç–µ–º—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
#     handlers = [logging.StreamHandler(sys.stdout)]
#     if log_file:
#         handlers.append(logging.FileHandler(log_file, encoding='utf-8'))
#
#     logging.basicConfig(
#         level=logging.INFO,
#         format='%(asctime)s - %(levelname)s - %(message)s',
#         handlers=handlers
#     )
#     # –£–º–µ–Ω—å—à–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ playwright
#     logging.getLogger('playwright').setLevel(logging.WARNING)
#
#
# BASE_URL = "https://lombard-perspectiva.ru"
# try:
#     USER_AGENT = UserAgent().random
# except Exception:
#     USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36"
# MAX_CONCURRENT_TASKS = 10  # –£–≤–µ–ª–∏—á–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
# REQUEST_TIMEOUT = 12_0000  # 30 —Å–µ–∫—É–Ω–¥ —Ç–∞–π–º–∞—É—Ç
#
#
# class ParserTimer:
#     """–ö–ª–∞—Å—Å –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""
#
#     def __init__(self):
#         self.start_time = None
#
#     def start(self):
#         self.start_time = time.perf_counter()
#         logger.info("‚è± –ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞")
#
#     def elapsed(self):
#         return time.perf_counter() - self.start_time
#
#
# async def block_resources(route):
#     """–ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –Ω–µ–Ω—É–∂–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏"""
#     if route.request.resource_type in {'image', 'stylesheet', 'font'}:
#         await route.abort()
#     else:
#         await route.continue_()
#
#
# async def get_product_data(context, url: str, semaphore: asyncio.Semaphore) -> dict | None:
#     """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–æ–¥—É–∫—Ç–µ"""
#     async with semaphore:
#         page = await context.new_page()
#         try:
#             # –ë–ª–æ–∫–∏—Ä—É–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã
#             await page.route('**/*.{png,jpg,jpeg,webp,svg,gif,css,woff2}', block_resources)
#
#             await page.goto(url, wait_until="domcontentloaded", timeout=REQUEST_TIMEOUT)
#
#             # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–∞–Ω–Ω—ã—Ö
#             product_data = await page.evaluate('''() => {
#                 const h1 = document.querySelector('h1[itemprop="name"]');
#                 if (!h1) return null;
#                 return {
#                     name: h1.textContent.trim(),
#                     url: window.location.href
#                 };
#             }''')
#
#             if product_data:
#                 logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {product_data['name'][:30]}...")
#                 return product_data
#             return None
#
#         except Exception as e:
#             logger.error(f"‚ùå –û—à–∏–±–∫–∞ ({url[-15:]}): {str(e)[:50]}...")
#             return None
#         finally:
#             await page.close()
#
#
# USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
# # –ú–æ–∂–Ω–æ –ø–æ–¥—Å—Ç–∞–≤–∏—Ç—å –ø—Ä–æ–∫—Å–∏, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ:
# PROXY = None  # –Ω–∞–ø—Ä–∏–º–µ—Ä: "http://login:pass@ip:port"
#
# async def get_product_links(page, page_num: int, retries=3) -> list:
#     url = f"{BASE_URL}/clocks_today/?page={page_num}"
#     logger.info(f"üåê –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
#
#     for attempt in range(retries):
#         try:
#             await page.goto(url, wait_until="domcontentloaded", timeout=120000)
#             await page.wait_for_selector('a.product-list-item', timeout=15000)
#
#             if await page.query_selector('div#recaptcha'):
#                 raise Exception("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∫–∞–ø—á–∞")
#
#             links = await page.evaluate('''() =>
#                 Array.from(document.querySelectorAll('a.product-list-item'))
#                 .map(a => a.href)
#                 .filter(Boolean)
#             ''')
#
#             found_links = [urljoin(BASE_URL, link) for link in links if link]
#             logger.info(f"üîó –ù–∞–π–¥–µ–Ω–æ {len(found_links)} —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}")
#             return found_links
#
#         except Exception as e:
#             logger.warning(f"‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
#             if attempt == retries - 1:
#                 logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
#                 return []
#
# async def parse_products_page(page_num: int, items_limit: int = None):
#     timer = ParserTimer()
#     timer.start()
#
#     async with async_playwright() as p:
#         launch_args = {
#             "headless": False,  # —á—Ç–æ–±—ã –≤–∏–¥–µ—Ç—å, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç
#             "args": [
#                 "--disable-blink-features=AutomationControlled",
#                 "--no-sandbox",
#                 "--disable-setuid-sandbox",
#                 "--disable-dev-shm-usage",
#                 "--disable-gpu",
#             ]
#         }
#
#         if PROXY:
#             launch_args["proxy"] = {"server": PROXY}
#
#         browser = await p.chromium.launch(**launch_args)
#
#         context = await browser.new_context(
#             user_agent=USER_AGENT,
#             java_script_enabled=True,
#             ignore_https_errors=True,
#             viewport={'width': 1280, 'height': 800},
#             locale='ru-RU',
#         )
#
#         main_page = await context.new_page()
#
#         try:
#             product_links = await get_product_links(main_page, page_num)
#             if items_limit:
#                 product_links = product_links[:items_limit]
#
#             semaphore = asyncio.Semaphore(5)
#
#             tasks = [
#                 get_product_data(context, link, semaphore)
#                 for link in product_links
#             ]
#             results_raw = await asyncio.gather(*tasks)
#             results = [r for r in results_raw if r]
#
#             elapsed = timer.stop()
#             logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {len(results)} —Ç–æ–≤–∞—Ä–æ–≤ –∑–∞ {timedelta(seconds=elapsed)}")
#             return results
#
#         except Exception as e:
#             logger.error(f"üî• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {str(e)}")
#             return []
#
#         finally:
#             await main_page.close()
#             await context.close()
#             await browser.close()
#
# async def cli_main(args):
#     """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è CLI"""
#     setup_logging(args.log_file)
#
#     try:
#         logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞: —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {args.page}, –ª–∏–º–∏—Ç {args.limit}")
#         results = await parse_products_page(
#             page_num=args.page,
#             items_limit=args.limit
#         )
#
#         print("\n" + "=" * 50)
#         print(f"–í—Å–µ–≥–æ –ø–æ–ª—É—á–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
#         for i, item in enumerate(results[:5], 1):  # –í—ã–≤–æ–¥–∏–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5 –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
#             print(f"{i}. {item['name']} - {item['url']}")
#         if len(results) > 5:
#             print(f"... –∏ –µ—â–µ {len(results) - 5} —Ç–æ–≤–∞—Ä–æ–≤")
#         print("=" * 50 + "\n")
#
#         return 0
#     except Exception as e:
#         logger.error(f"üí• –û—à–∏–±–∫–∞: {str(e)}", exc_info=True)
#         return 1
#     finally:
#         logger.info("üõë –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø–∞—Ä—Å–µ—Ä–∞")
#
#
# def parse_cli_args():
#     """–†–∞–∑–±–æ—Ä –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
#     parser = argparse.ArgumentParser(
#         description="üïµÔ∏è –ü–∞—Ä—Å–µ—Ä —Å–∞–π—Ç–∞ lombard-perspectiva.ru",
#         formatter_class=argparse.ArgumentDefaultsHelpFormatter
#     )
#     parser.add_argument(
#         '--page',
#         type=int,
#         default=1,
#         help='–ù–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞'
#     )
#     parser.add_argument(
#         '--limit',
#         type=int,
#         default=None,
#         help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞'
#     )
#     parser.add_argument(
#         '--log-file',
#         type=str,
#         default=None,
#         help='–§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª–æ–≥–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)'
#     )
#     return parser.parse_args()
#
#
# if __name__ == "__main__":
#     args = parse_cli_args()
#     sys.exit(asyncio.run(cli_main(args)))



import asyncio
from tqdm.asyncio import tqdm
from playwright.async_api import async_playwright
from urllib.parse import urljoin
import logging
import time
from typing import Optional
from datetime import timedelta
from fake_useragent import UserAgent
import argparse
import sys
import tqdm

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def setup_logging(log_file=None):
    handlers = [logging.StreamHandler(sys.stdout)]
    if log_file:
        handlers.append(logging.FileHandler(log_file, encoding='utf-8'))

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=handlers
    )
    logging.getLogger('playwright').setLevel(logging.WARNING)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
BASE_URL = "https://lombard-perspectiva.ru"
try:
    USER_AGENT = UserAgent().random
except Exception:
    USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36"

MAX_CONCURRENT_TASKS = 10
REQUEST_TIMEOUT = 120_000  # 2 –º–∏–Ω
HEADLESS = True
VIEWPORT = {'width': 600, 'height': 400}
PROXY = None  # –ø—Ä–∏–º–µ—Ä: "http://login:pass@ip:port"

class ParserTimer:
    def __init__(self):
        self.start_time = None

    def start(self):
        self.start_time = time.perf_counter()
        logger.info("‚è± –ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞")

    def stop(self):
        return time.perf_counter() - self.start_time

async def block_resources(route):
    # 'image'
    if route.request.resource_type in {'stylesheet', 'font'}:
        await route.abort()
    else:
        await route.continue_()

async def get_product_data(context, url: str, semaphore: asyncio.Semaphore) -> Optional[dict]:
    async with semaphore:
        page = await context.new_page()
        try:
            # await page.route('**/*.{png,jpg,jpeg,webp,svg,gif,css,woff2}', block_resources)
            await page.goto(url, wait_until="domcontentloaded", timeout=REQUEST_TIMEOUT)
            await page.wait_for_selector('img[itemprop="image"]', timeout=15000)

            product_data = await page.evaluate('''() => {
                const h1 = document.querySelector('h1[itemprop="name"]');
                const img = document.querySelector('img[itemprop="image"]');
                if (!h1 || !img) return null;

                const src = img.currentSrc || img.src || img.getAttribute('data-src') || '';

                return {
                    name: h1.textContent.trim(),
                    url: window.location.href,
                    image: src
                };
            }''')

            # –ü–æ–¥—Å—á—ë—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å–∞

            if product_data:
                logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {product_data['name'][:30]}...")
                return product_data
            return None

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ ({url[-15:]}): {str(e)[:50]}...")
            return None
        finally:
            await page.close()

async def get_product_links(page, page_num: int, retries=3) -> list:
    url = f"{BASE_URL}/clocks_today/?page={page_num}"
    logger.info(f"üåê –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")

    for attempt in range(retries):
        try:
            await page.goto(url, wait_until="domcontentloaded", timeout=REQUEST_TIMEOUT)
            await page.wait_for_selector('a.product-list-item', timeout=15000)

            if await page.query_selector('div#recaptcha'):
                raise Exception("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∫–∞–ø—á–∞")

            links = await page.evaluate('''() =>
                Array.from(document.querySelectorAll('a.product-list-item'))
                    .map(a => a.href)
                    .filter(Boolean)
            ''')

            found_links = [urljoin(BASE_URL, link) for link in links if link]
            logger.info(f"üîó –ù–∞–π–¥–µ–Ω–æ {len(found_links)} —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}")
            return found_links

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
            if attempt == retries - 1:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
                return []

async def parse_products_page(page_num: int, items_limit: int = None, on_progress=None):
    timer = ParserTimer()
    timer.start()

    async with async_playwright() as p:
        launch_args = {
            "headless": HEADLESS,
            "args": [
                "--disable-blink-features=AutomationControlled",
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-gpu",
            ]
        }
        if PROXY:
            launch_args["proxy"] = {"server": PROXY}

        browser = await p.chromium.launch(**launch_args)
        context = await browser.new_context(
            user_agent=USER_AGENT,
            java_script_enabled=True,
            ignore_https_errors=True,
            viewport=VIEWPORT,
            locale='ru-RU',
        )

        main_page = await context.new_page()
        try:
            product_links = await get_product_links(main_page, page_num)
            if items_limit:
                product_links = product_links[:items_limit]

            semaphore = asyncio.Semaphore(MAX_CONCURRENT_TASKS)
            # –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –ø–æ —Å—Å—ã–ª–∫–∞–º
            tasks = [get_product_data(context, link, semaphore) for link in product_links]
            # tqdm.gather(*tasks) –∑–∞–º–µ–Ω—è–µ—Ç –æ–±—ã—á–Ω—ã–π asyncio.gather, –Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å.
            # ncols=80 ‚Äî —à–∏—Ä–∏–Ω–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞, –º–æ–∂–µ—à—å –ø–æ–¥–æ–≥–Ω–∞—Ç—å –ø–æ–¥ –∫–æ–Ω—Å–æ–ª—å.
            # desc ‚Äî –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞.
            try:
                results_raw = await asyncio.gather(*tasks)
                # results_raw = await tqdm.gather(*tasks, desc="üì¶ –ü–∞—Ä—Å–∏–Ω–≥", ncols=80)
            except RuntimeError as e:
                logger.warning(f"Async error (interpreter shutdown?): {e}")
                results_raw = []
            # result_raw = await tqdm.gather(*tasks, desk="–ü–∞—Ä—Å–∏–Ω–≥ —Ç–æ–≤–∞—Ä–æ–≤", ncols=80)

            results = [r for r in results_raw if r]

            elapsed = timer.stop()
            logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {len(results)} —Ç–æ–≤–∞—Ä–æ–≤ –∑–∞ {timedelta(seconds=elapsed)}")
            return results

        except Exception as e:
            logger.error(f"üî• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {str(e)}")
            return []

        finally:
            await main_page.close()
            await context.close()
            await browser.close()

async def cli_main(args):
    setup_logging(args.log_file)

    try:
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞: —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {args.page}, –ª–∏–º–∏—Ç {args.limit}")
        results = await parse_products_page(page_num=args.page, items_limit=args.limit)

        print("\n" + "=" * 50)
        print(f"–í—Å–µ–≥–æ –ø–æ–ª—É—á–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
        for i, item in enumerate(results[:5], 1):
            print(f"""{i}.
            <div class="catalog-item">
              <img src="{item['image']}" alt="{item['name']}" loading="lazy" itemprop="image" class="catalog-item-img--object">
              <h3>{item['name']}</h3>
              <a href="{item['url']}">–°–º–æ—Ç—Ä–µ—Ç—å</a>
            </div>""")
        # {item['name']} - {item['url']}")
        # if len(results) > 5:
        #     print(f"... –∏ –µ—â–µ {len(results) - 5} —Ç–æ–≤–∞—Ä–æ–≤")
        # print("=" * 50 + "\n")

        return 0

    except Exception as e:
        logger.error(f"üí• –û—à–∏–±–∫–∞: {str(e)}", exc_info=True)
        return 1
    finally:
        logger.info("üõë –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø–∞—Ä—Å–µ—Ä–∞")

def parse_cli_args():
    parser = argparse.ArgumentParser(
        description="üïµÔ∏è –ü–∞—Ä—Å–µ—Ä —Å–∞–π—Ç–∞ lombard-perspectiva.ru",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument('--page', type=int, default=1, help='–ù–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞')
    parser.add_argument('--limit', type=int, default=None, help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞')
    parser.add_argument('--log-file', type=str, default=None, help='–§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª–æ–≥–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)')
    return parser.parse_args()

if __name__ == "__main__":
    setup_logging()
    args = parse_cli_args()
    sys.exit(asyncio.run(cli_main(args)))



 # try:
 #            product_links = await get_product_links(main_page, page_num)
 #            if items_limit:
 #                product_links = product_links[:items_limit]
 #
 #            semaphore = asyncio.Semaphore(MAX_CONCURRENT_TASKS)
 #
 #            results = []
 #            total_links = len(product_links)
 #
 #            for idx, link in enumerate(product_links):
 #                result = await get_product_data(context, link, semaphore)
 #                if result:
 #                    results.append(result)
 #                if on_progress:
 #                    await on_progress(idx + 1, total_links)
 #
 #            elapsed = timer.stop()
 #            logger.info(f"üìä –°–ø–∞—Ä—à–µ–Ω–æ: {len(results)} —Ç–æ–≤–∞—Ä–æ–≤ –∑–∞ {timedelta(seconds=elapsed)}")
 #            return results
 #
 #        except Exception as e:
 #            logger.error(f"üî• –û—à–∏–±–∫–∞: {str(e)}")
 #            return []
 #
 #        finally:
 #            await main_page.close()
 #            await context.close()
 #            await browser.close()



# import asyncio
# from playwright.async_api import async_playwright
# from urllib.parse import urljoin
# import logging
# import time
# from datetime import timedelta
# from fake_useragent import UserAgent
#
# # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(levelname)s - %(message)s'
# )
# logger = logging.getLogger(__name__)
#
# BASE_URL = "https://lombard-perspectiva.ru"
# USER_AGENT = UserAgent().random
#
#
# class ParserTimer:
#     def __init__(self):
#         self.start_time = None
#         self.end_time = None
#
#     def start(self):
#         self.start_time = time.time()
#         logger.info("‚è± –ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞")
#
#     def stop(self):
#         self.end_time = time.time()
#         elapsed = self.end_time - self.start_time
#         logger.info(f"‚è± –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –ó–∞—Ç—Ä–∞—á–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è: {timedelta(seconds=elapsed)}")
#         return elapsed
#
#
# async def get_product_data(context, url: str, semaphore: asyncio.Semaphore) -> dict | None:
#     async with semaphore:
#         page = await context.new_page()
#         try:
#             await page.goto(url, wait_until="domcontentloaded", timeout=80000)
#             h1 = await page.wait_for_selector('h1[itemprop="name"]', timeout=15000)
#             name = await h1.text_content()
#             logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –ø—Ä–æ–¥—É–∫—Ç: {name[:30]}...")
#             return {'name': name.strip(), 'url': url}
#         except Exception as e:
#             logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {str(e)}")
#             return None
#         finally:
#             await page.close()
#
#
# async def get_product_links(page, page_num: int, retries=2) -> list:
#     url = f"{BASE_URL}/clocks_today/?page={page_num}"
#     logger.info(f"üåê –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
#
#     for attempt in range(retries):
#         try:
#             await page.goto(url, wait_until="load", timeout=70000)
#             await page.wait_for_selector('a.product-list-item', timeout=15000)
#
#             if await page.query_selector('div#recaptcha'):
#                 raise Exception("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∫–∞–ø—á–∞")
#
#             links = await page.evaluate('''() =>
#                 Array.from(document.querySelectorAll('a.product-list-item'))
#                 .map(a => a.href)
#                 .filter(Boolean)
#                 ''')
#
#             # links = await page.eval_on_selector_all(
#             #     'a.product-list-item',
#             #     "elements => elements.map(el => el.href)"
#             # )
#             found_links = [urljoin(BASE_URL, link) for link in links if link]
#             logger.info(f"üîó –ù–∞–π–¥–µ–Ω–æ {len(found_links)} —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}")
#             return found_links
#         except Exception as e:
#             logger.warning(f"‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
#             if attempt == retries - 1:
#                 logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
#                 return []
#             # await asyncio.sleep(.4)  # –ø–∞—É–∑–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
#
#
# async def parse_products_page(page_num: int, items_limit: int = None):
#     timer = ParserTimer()
#     timer.start()
#
#     async with async_playwright() as p:
#         browser = await p.chromium.launch(
#             headless=True,
#             args=[
#                 "--disable-blink-features=AutomationControlled",
#                 "--no-sandbox",
#                 "--disable-setuid-sandbox",
#                 "--disable-dev-shm-usage",
#                 "--disable-gpu",
#             ]
#         )
#
#         context = await browser.new_context(
#             user_agent=USER_AGENT,
#             java_script_enabled=True,
#             ignore_https_errors=True,
#             viewport={'width': 1280, 'height': 800},
#             locale='ru-RU',
#         )
#
#         main_page = await context.new_page()
#
#         try:
#             product_links = await get_product_links(main_page, page_num)
#             if items_limit:
#                 product_links = product_links[:items_limit]
#
#             semaphore = asyncio.Semaphore(5)  # –º–∞–∫—Å–∏–º—É–º 5 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
#
#             tasks = [
#                 get_product_data(context, link, semaphore)
#                 for link in product_links
#             ]
#             results_raw = await asyncio.gather(*tasks)
#
#             results = [r for r in results_raw if r]
#
#             elapsed = timer.stop()
#             logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {len(results)} —Ç–æ–≤–∞—Ä–æ–≤ –∑–∞ {timedelta(seconds=elapsed)}")
#             return results
#
#         except Exception as e:
#             logger.error(f"üî• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {str(e)}")
#             return []
#
#         finally:
#             await main_page.close()
#             await context.close()
#             await browser.close()
#
#
# async def main():
#     try:
#         logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞")
#         results = await parse_products_page(page_num=1, items_limit=20)
#
#         print("\n" + "=" * 50)
#         print(f"–í—Å–µ–≥–æ –ø–æ–ª—É—á–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
#         for i, item in enumerate(results, 1):
#             print(f"{i}. {item['name']} - {item['url']}")
#         print("=" * 50 + "\n")
#
#     except Exception as e:
#         logger.error(f"üí• –û—à–∏–±–∫–∞ –≤ main: {str(e)}")
#     finally:
#         logger.info("üõë –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø–∞—Ä—Å–µ—Ä–∞")
#
#
# if __name__ == "__main__":
#     asyncio.run(main())

# import asyncio
# from playwright.async_api import async_playwright
# from urllib.parse import urljoin
# import logging
# import time
# from datetime import timedelta
# from fake_useragent import UserAgent
#
# # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(levelname)s - %(message)s'
# )
# logger = logging.getLogger(__name__)
#
# BASE_URL = "https://lombard-perspectiva.ru"
# USER_AGENT = UserAgent().random
# MAX_RETRIES = 3  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
# TIMEOUT = 30000  # 30 —Å–µ–∫—É–Ω–¥ —Ç–∞–π–º–∞—É—Ç
#
#
# class ParserTimer:
#     def __init__(self):
#         self.start_time = None
#
#     def start(self):
#         self.start_time = time.time()
#         logger.info("‚è± –ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞")
#
#     def elapsed(self):
#         return timedelta(seconds=time.time() - self.start_time)
#
#
# async def get_product_data(context, url: str, semaphore: asyncio.Semaphore) -> dict | None:
#     async with semaphore:
#         page = await context.new_page()
#         try:
#             # –£—Å–∫–æ—Ä–µ–Ω–∏–µ: –æ—Ç–∫–ª—é—á–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –Ω–µ–Ω—É–∂–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤
#             await page.route('**/*.{png,jpg,jpeg,webp,svg,gif,css,woff2}', lambda route: route.abort())
#
#             await page.goto(url, wait_until="domcontentloaded", timeout=TIMEOUT)
#
#             # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–∞–Ω–Ω—ã—Ö
#             product_data = await page.evaluate('''() => {
#                 const h1 = document.querySelector('h1[itemprop="name"]');
#                 if (!h1) return null;
#                 return {
#                     name: h1.textContent.trim(),
#                     url: window.location.href
#                 };
#             }''')
#
#             if product_data:
#                 logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {product_data['name'][:30]}...")
#                 return product_data
#             return None
#
#         except Exception as e:
#             logger.error(f"‚ùå –û—à–∏–±–∫–∞ ({url[-15:]}): {str(e)[:50]}...")
#             return None
#         finally:
#             await page.close()
#
#
# async def get_product_links(page, page_num: int) -> list:
#     url = f"{BASE_URL}/clocks_today/?page={page_num}"
#
#     for attempt in range(MAX_RETRIES):
#         try:
#             # –£—Å–∫–æ—Ä–µ–Ω–∏–µ: networkidle –≤–º–µ—Å—Ç–æ load
#             await page.goto(url, wait_until="networkidle", timeout=TIMEOUT)
#
#             # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
#             if await page.query_selector('div#recaptcha'):
#                 raise Exception("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∫–∞–ø—á–∞")
#
#             links = await page.evaluate('''() =>
#                 Array.from(document.querySelectorAll('a.product-list-item'))
#                     .map(a => a.href)
#                     .filter(Boolean)
#             ''')
#
#             found_links = [urljoin(BASE_URL, link) for link in links]
#             logger.info(f"üîó –ù–∞–π–¥–µ–Ω–æ {len(found_links)} —Ç–æ–≤–∞—Ä–æ–≤")
#             return found_links
#
#         except Exception as e:
#             logger.warning(f"‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –Ω–µ —É–¥–∞–ª–∞—Å—å: {str(e)[:50]}...")
#             if attempt == MAX_RETRIES - 1:
#                 logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num} –ø–æ—Å–ª–µ {MAX_RETRIES} –ø–æ–ø—ã—Ç–æ–∫")
#                 return []
#             await asyncio.sleep(2 * (attempt + 1))  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
#
#
# async def parse_products_page(page_num: int, items_limit: int = None):
#     timer = ParserTimer()
#     timer.start()
#
#     async with async_playwright() as p:
#         # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—É—Å–∫–∞
#         browser = await p.chromium.launch(
#             headless=True,
#             args=[
#                 "--disable-web-security",
#                 "--single-process",
#                 "--disable-blink-features=AutomationControlled",
#                 "--no-sandbox",
#                 "--disable-setuid-sandbox"
#             ]
#         )
#
#         context = await browser.new_context(
#             user_agent=USER_AGENT,
#             java_script_enabled=True,
#             ignore_https_errors=True,
#             viewport={'width': 1280, 'height': 800},
#             locale='ru-RU'
#         )
#
#         page = await context.new_page()
#
#         try:
#             product_links = await get_product_links(page, page_num)
#             if items_limit and items_limit > 0:
#                 product_links = product_links[:items_limit]
#
#             logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ {len(product_links)} —Ç–æ–≤–∞—Ä–æ–≤...")
#
#             semaphore = asyncio.Semaphore(10)  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–æ 10 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
#
#             tasks = [get_product_data(context, link, semaphore) for link in product_links]
#             results = await asyncio.gather(*tasks)
#
#             valid_results = [r for r in results if r]
#             elapsed = timer.elapsed()
#
#             logger.info(f"‚è± –ó–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {elapsed}")
#             logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {len(valid_results)}/{len(product_links)}")
#
#             return valid_results
#         finally:
#             await page.close()
#             await context.close()
#             await browser.close()
#
#
# async def main():
#     try:
#         logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞")
#         results = await parse_products_page(page_num=1, items_limit=20)
#
#         print(f"\n–£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ: {len(results)} —Ç–æ–≤–∞—Ä–æ–≤")
#         if results:
#             print(f"–ü–µ—Ä–≤—ã–π —Ç–æ–≤–∞—Ä: {results[0]['name'][:50]}...")
#     except Exception as e:
#         logger.error(f"üí• –û—à–∏–±–∫–∞: {str(e)}")
#
#
# if __name__ == "__main__":
#     asyncio.run(main())




# import asyncio
# from playwright.async_api import async_playwright
# from urllib.parse import urljoin
# import logging
# import time
# from datetime import timedelta
# from fake_useragent import UserAgent
#
# # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(levelname)s - %(message)s'
# )
# logger = logging.getLogger(__name__)
#
# BASE_URL = "https://lombard-perspectiva.ru"
# USER_AGENT = UserAgent().random
#
# class ParserTimer:
#     def __init__(self):
#         self.start_time = None
#         self.end_time = None
#
#     def start(self):
#         self.start_time = time.time()
#         logger.info("‚è± –ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞")
#
#     def stop(self):
#         self.end_time = time.time()
#         elapsed = self.end_time - self.start_time
#         logger.info(f"‚è± –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –ó–∞—Ç—Ä–∞—á–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è: {timedelta(seconds=elapsed)}")
#         return elapsed
#
#
# async def get_product_data(page, url: str) -> str:
#     try:
#         await page.goto(url, wait_until="domcontentloaded", timeout=20000)
#         h1 = await page.wait_for_selector('h1[itemprop="name"]', timeout=15000)
#         name = await h1.text_content()
#         logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –ø—Ä–æ–¥—É–∫—Ç: {name[:30]}...")
#         return name.strip()
#     except Exception as e:
#         logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {str(e)}")
#         return None
#
#
# async def get_product_links(page, page_num: int, retries=2) -> list:
#     url = f"{BASE_URL}/clocks_today/?page={page_num}"
#     logger.info(f"üåê –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
#
#     for attempt in range(retries):
#         try:
#             await page.goto(url, wait_until="load", timeout=40000)
#             await page.wait_for_selector('a.product-list-item', timeout=15000)
#
#             links = await page.eval_on_selector_all(
#                 'a.product-list-item',
#                 "elements => elements.map(el => el.href)"
#             )
#             found_links = [urljoin(BASE_URL, link) for link in links if link]
#             logger.info(f"üîó –ù–∞–π–¥–µ–Ω–æ {len(found_links)} —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}")
#             return found_links
#         except Exception as e:
#             logger.warning(f"‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
#             if attempt == retries - 1:
#                 logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
#                 return []
#             await asyncio.sleep(2)  # –ø–∞—É–∑–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
#
#
# async def parse_products_page(page_num: int, items_limit: int = None):
#     timer = ParserTimer()
#     timer.start()
#
#     async with async_playwright() as p:
#         browser = await p.chromium.launch(
#             headless=True,
#             args=[
#                 "--disable-blink-features=AutomationControlled",
#                 "--no-sandbox",
#                 "--disable-setuid-sandbox",
#                 "--disable-dev-shm-usage",
#                 "--disable-gpu",
#             ]
#         )
#
#         context = await browser.new_context(
#             user_agent=USER_AGENT,
#             java_script_enabled=True,
#             ignore_https_errors=True,
#             viewport={'width': 1280, 'height': 800},
#             locale='ru-RU',
#         )
#
#         page = await context.new_page()
#
#         try:
#             product_links = await get_product_links(page, page_num)
#             if items_limit:
#                 product_links = product_links[:items_limit]
#
#             results = []
#             for i, link in enumerate(product_links, 1):
#                 logger.info(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–≤–∞—Ä–∞ {i}/{len(product_links)}")
#                 product_data = await get_product_data(page, link)
#                 if product_data:
#                     results.append({
#                         'name': product_data,
#                         'url': link
#                     })
#                 await asyncio.sleep(0.5)
#
#             elapsed = timer.stop()
#             logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {len(results)} —Ç–æ–≤–∞—Ä–æ–≤ –∑–∞ {timedelta(seconds=elapsed)}")
#             return results
#         except Exception as e:
#             logger.error(f"üî• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {str(e)}")
#             return []
#         finally:
#             await page.close()
#             await context.close()
#             await browser.close()
#
#
# async def main():
#     try:
#         logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞")
#         results = await parse_products_page(page_num=1, items_limit=5)
#
#         print("\n" + "=" * 50)
#         print(f"–í—Å–µ–≥–æ –ø–æ–ª—É—á–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
#         for i, item in enumerate(results, 1):
#             print(f"{i}. {item['name']} - {item['url']}")
#         print("=" * 50 + "\n")
#
#     except Exception as e:
#         logger.error(f"üí• –û—à–∏–±–∫–∞ –≤ main: {str(e)}")
#     finally:
#         logger.info("üõë –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø–∞—Ä—Å–µ—Ä–∞")
#
#
# if __name__ == "__main__":
#     asyncio.run(main())












#
# import asyncio
# from playwright.async_api import async_playwright
# from urllib.parse import urljoin
# import logging
# import time
# from datetime import timedelta
# from fake_useragent import UserAgent
#
# # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(levelname)s - %(message)s'
# )
# logger = logging.getLogger(__name__)
#
# BASE_URL = "https://lombard-perspectiva.ru"
# USER_AGENT = UserAgent().random
#
#
#
# class ParserTimer:
#     def __init__(self):
#         self.start_time = None
#         self.end_time = None
#
#     def start(self):
#         self.start_time = time.time()
#         logger.info("‚è± –ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞")
#
#     def stop(self):
#         self.end_time = time.time()
#         elapsed = self.end_time - self.start_time
#         logger.info(f"‚è± –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –ó–∞—Ç—Ä–∞—á–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è: {timedelta(seconds=elapsed)}")
#         return elapsed
#
#
# async def get_product_data(page, url: str) -> str:
#     try:
#         await page.goto(url, wait_until="domcontentloaded", timeout=20000)
#         h1 = await page.wait_for_selector('h1[itemprop="name"]', timeout=15000)
#         name = await h1.text_content()
#         logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –ø—Ä–æ–¥—É–∫—Ç: {name[:30]}...")
#         return name.strip()
#     except Exception as e:
#         logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {str(e)}")
#         return None
#
#
# async def get_product_links(page, page_num: int) -> list:
#     url = f"{BASE_URL}/clocks_today/?page={page_num}"
#     logger.info(f"üåê –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
#
#     try:
#         await page.goto(url, wait_until="domcontentloaded", timeout=20000)
#         await page.wait_for_selector('a.product-list-item', timeout=15000)
#
#         links = await page.eval_on_selector_all(
#             'a.product-list-item',
#             "elements => elements.map(el => el.href)"
#         )
#         found_links = [urljoin(BASE_URL, link) for link in links if link]
#         logger.info(f"üîó –ù–∞–π–¥–µ–Ω–æ {len(found_links)} —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}")
#         return found_links
#     except Exception as e:
#         logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {str(e)}")
#         return []
#
#
# async def parse_products_page(page_num: int, items_limit: int = None):
#     timer = ParserTimer()
#     timer.start()
#
#     async with async_playwright() as p:
#         browser = await p.chromium.launch(
#             headless=True,
#             args=[
#                 "--disable-blink-features=AutomationControlled",
#                 "--no-sandbox",
#                 "--disable-setuid-sandbox",
#                 "--disable-dev-shm-usage",
#                 "--disable-gpu",
#             ]
#         )
#
#         context = await browser.new_context(
#             user_agent=USER_AGENT,
#             java_script_enabled=True,
#             ignore_https_errors=True,
#         )
#
#         page = await context.new_page()
#
#         try:
#             product_links = await get_product_links(page, page_num)
#             if items_limit:
#                 product_links = product_links[:items_limit]
#
#             results = []
#             for i, link in enumerate(product_links, 1):
#                 logger.info(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–≤–∞—Ä–∞ {i}/{len(product_links)}")
#                 product_data = await get_product_data(page, link)
#                 if product_data:
#                     results.append({
#                         'name': product_data,
#                         'url': link
#                     })
#                 await asyncio.sleep(0.3)
#
#             elapsed = timer.stop()
#             logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {len(results)} —Ç–æ–≤–∞—Ä–æ–≤ –∑–∞ {timedelta(seconds=elapsed)}")
#             return results
#         except Exception as e:
#             logger.error(f"üî• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {str(e)}")
#             return []
#         finally:
#             await page.close()
#             await context.close()
#             await browser.close()
#
#
# async def main():
#     try:
#         logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞")
#         results = await parse_products_page(page_num=1, items_limit=5)
#
#         print("\n" + "=" * 50)
#         print(f"–í—Å–µ–≥–æ –ø–æ–ª—É—á–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
#         for i, item in enumerate(results, 1):
#             print(f"{i}. {item['name']} - {item['url']}")
#         print("=" * 50 + "\n")
#
#     except Exception as e:
#         logger.error(f"üí• –û—à–∏–±–∫–∞ –≤ main: {str(e)}")
#     finally:
#         logger.info("üõë –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø–∞—Ä—Å–µ—Ä–∞")
#
#
# if __name__ == "__main__":
#     asyncio.run(main())






